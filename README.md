# Data Engineering Final Project

> **ğŸ†•Â JulyÂ 2025Â UpdateÂ 2Â â€“ EmailÂ Analytics Pipeline**
>
> A brandâ€‘new **email pipeline** is live: Iceberg DDL forÂ Bronze â†’â€¯Silverâ€¯â†’â€¯Gold plus oneâ€‘shot CSV loaders. See **Whatâ€™s new** â¬‡ï¸

---

## Whatâ€™s new

| Area                | Highlights                                                                                                                                                                                                                                                                                                                                           |
| :------------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Streaming stack** | <ul><li>ğŸ“¦Â `streaming/` compose spins KafkaÂ +Â ZookeeperÂ +Â Kafkaâ€‘UI.</li><li>ğŸª„Â Producers forâ€¯*salesâ€‘events*,â€¯*equipmentâ€‘metrics*,â€¯*inventoryâ€‘updates*Â **and**â€¯*emailâ€‘send*.</li></ul>                                                                                                                                                                |
| **Spark jobs**      | <ul><li>ğŸš°Â `stream_to_bronze.py` â€“ universal StructuredÂ Streaming sink.</li><li>âœ‰ï¸Â `ingest_email_stream.py` â€“ dedicated live pipeline for email events.</li><li>ğŸ—ï¸Â `init_email_iceberg_tables.py` â€“ creates all Email tables (Bronze/Silver/Gold).</li><li>ğŸ“¥Â `load_bronze_email_csvs.py` â€“ bulkâ€‘loads historical email CSVs into Bronze.</li></ul> |
| **AirflowÂ 3 DAGs**  | <ul><li>ğŸ†•Â `email_stream_ingest` â€“ DockerOperator wrapper for the live email stream.</li><li>ğŸ†•Â `email_csv_bootstrap` â€“ runs the two new jobs once per env.</li></ul>                                                                                                                                                                                |
| **Makefile**        | <ul><li>`make init-email` â€“ runs the Iceberg DDL + CSV loader endâ€‘toâ€‘end.</li><li>Existing targets (`producers`, `streaming`, `demo`) unchanged.</li></ul>                                                                                                                                                                                           |
| **Compose tweaks**  | Added readâ€‘only mount `../processing/data â†’Â /opt/spark-apps/data` so Spark can see bootstrap CSVs.                                                                                                                                                                                                                                                   |

---

## 0Â Prerequisites

| Tool                 | Version       | Notes                              |
| -------------------- | ------------- | ---------------------------------- |
| DockerÂ Desktop       | 24Â or newer   | Linux: DockerÂ EngineÂ +Â ComposeÂ v2  |
| DockerÂ ComposeÂ v2    | bundled       | `docker compose version` to verify |
| Git                  | any recent    | â€“                                  |
| (WindowsÂ only)Â WSLâ€¯2 | UbuntuÂ 20.04+ | best I/O perf                      |

---

## 1Â Clone the project

```bash
# choose any folder you like
git clone https://github.com/<yourâ€‘org>/data-eng-project.git
cd data-eng-project
```

---

## 2Â Create your personal `.env`

Copy the template and adjust `DATA_ROOT` â€“ everything else works outâ€‘ofâ€‘theâ€‘box.

```bash
cp .env.example .env
```

```dotenv
DATA_ROOT=/abs/path/to/data-eng-project
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=password
AIRFLOW_UID=1000
AIRFLOW_PROJ_DIR=${DATA_ROOT}/orchestration
```

---

## 3Â Oneâ€‘time network

```bash
docker network create data_eng_net   # safe if it exists
```

---

## 4Â Spin up the stack

```bash
# Storage + Spark + Iceberg
docker compose -f processing/docker-compose.yml --env-file .env up -d

# Streaming (Kafka)
docker compose -f streaming/docker-compose.yml  --env-file .env up -d

# Orchestration (AirflowÂ 3)
docker compose -f orchestration/docker-compose.yml --env-file .env up -d
```

> **Note:** `processing/docker-compose.yml` now mounts `../processing/data` into the Spark client container at `/opt/spark-apps/data` (readâ€‘only) â€“ required for the email CSV bootstrap.

---

### 4.1Â Initialise the Email Iceberg schema and seed Bronze

Run once per environment (or use `make init-email`).

```bash
# create tables
docker exec -it spark-submit spark-submit /opt/jobs/init_email_iceberg_tables.py

# load historical CSVs into Bronze
docker exec -it spark-submit spark-submit /opt/jobs/load_bronze_email_csvs.py
```

You should seeÂ â‰ˆ100Â rows in each `local.bronze.*` table in the Spark console.

---

## 5Â WebÂ UIs & credentials

| Component     | URL                                                      | Default creds       | Notes                     |
| ------------- | -------------------------------------------------------- | ------------------- | ------------------------- |
| AirflowÂ 3     | [http://localhost:8080/home](http://localhost:8080/home) | `airflow / airflow` | open `/home` not `/`      |
| MinIOÂ Console | [http://localhost:9001](http://localhost:9001)           | `admin / password`  | change in `.env`          |
| SparkÂ UI      | [http://localhost:4040](http://localhost:4040)           | â€“                   | only while job is running |
| IcebergÂ REST  | [http://localhost:8181](http://localhost:8181)           | â€“                   | programmatic              |
| KafkaÂ UI      | [http://localhost:8090](http://localhost:8090)           | â€“                   | browse topics             |

---

## 6Â GettingÂ Started â€“ Streaming edition ğŸ›°ï¸

```bash
make start        # builds & boots full stack
make init-email   # run the Iceberg DDL + CSV loader
make producers    # start Kafka producers (sales, metrics, inventory, email)
make streaming    # launch Spark Structured Streaming consumer
```

* Monitor Spark â†’Â Streaming tab
* Check MinIO â†’Â `bronze/` Iceberg files
* Trigger DAG `24sender_batch_etl` in Airflow to push Bronze â†’ Silver â†’ Gold

---

## 7Â Stopping / cleaning up

```bash
make stop         # stops all compose stacks
make clean        # removes **all** named volumes â€“ destructive!
```

---

## 8Â Troubleshooting

* **404 on SparkÂ UI** â€“ open `<host>:4040` only while a job is active.
* **Airflow connection reset** â€“ always point browser to `/home`.
* **CatalogNotFoundException (`local` vs `spark_catalog`)** â€“ ensure the extra `.config("spark.sql.catalog.local â€¦)` lines are present in every `create_spark_session()`.
* **CSV loader canâ€™t find files** â€“ verify `processing/data/bronze_*` exist and the volume mount is in place.

---

## 9Â Reference docs

See [`docs/`](docs/) for architecture diagrams, data models, and a full walkthrough.

Happy hackingÂ &Â may your streams be everâ€‘flowing! ğŸš€
